{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/admhan/Airiau/blob/main/AdamHannachi_AnaelErnadote_CarRacing_RL_IASD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc5ukRcu_RK0"
      },
      "source": [
        "# Proximal Policy Optimitization (PPO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7ahHxEeGFZ5"
      },
      "source": [
        "# Installation et Import"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "TiGj6P-NGXIp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJfGFgyL4S84",
        "outputId": "9b163841-5737-45e9-dbb0-b104aaac42e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74HCvC9D4thW",
        "outputId": "96ff03a6-4d16-4a74-dbe1-0a9c4ec147be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.7.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (2.9.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d==2.3.10 (from gymnasium[box2d])\n",
            "  Downloading Box2D-2.3.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable_baselines3) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.3)\n",
            "Downloading stable_baselines3-2.7.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Box2D-2.3.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: box2d, stable_baselines3\n",
            "Successfully installed box2d-2.3.10 stable_baselines3-2.7.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install stable_baselines3 gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWTxFSFNGFZ6"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fApW3L294u-g",
        "outputId": "a63a537e-be32-4e1d-adca-3dc99a42dbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# =========================\n",
        "# Stable-Baselines3 callbacks & evaluation\n",
        "# =========================\n",
        "from stable_baselines3.common.callbacks import (\n",
        "    EvalCallback,\n",
        "    CallbackList,\n",
        "    CheckpointCallback\n",
        ")\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "\n",
        "# =========================\n",
        "# Utilities\n",
        "# =========================\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "from google.colab import files\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fonction"
      ],
      "metadata": {
        "id": "gXcKaRCTGkRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO"
      ],
      "metadata": {
        "id": "BIrQ6iMZGmmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import warnings\n",
        "from typing import Any, ClassVar, TypeVar\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch.nn import functional as F\n",
        "from gymnasium import spaces\n",
        "\n",
        "from stable_baselines3.common.buffers import RolloutBuffer\n",
        "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
        "from stable_baselines3.common.policies import (\n",
        "    ActorCriticCnnPolicy,\n",
        "    ActorCriticPolicy,\n",
        "    BasePolicy,\n",
        "    MultiInputActorCriticPolicy,\n",
        ")\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
        "from stable_baselines3.common.utils import explained_variance\n",
        "\n",
        "\n",
        "SelfPPO = TypeVar(\"SelfPPO\", bound=\"PPO\")\n",
        "\n",
        "\n",
        "class PPO(OnPolicyAlgorithm):\n",
        "    \"\"\"\n",
        "    Proximal Policy Optimization (PPO) ‚Äî clip variant (SB3-style).\n",
        "    \"\"\"\n",
        "\n",
        "    policy_aliases: ClassVar[dict[str, type[BasePolicy]]] = {\n",
        "        \"MlpPolicy\": ActorCriticPolicy,\n",
        "        \"CnnPolicy\": ActorCriticCnnPolicy,\n",
        "        \"MultiInputPolicy\": MultiInputActorCriticPolicy,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: str | type[ActorCriticPolicy],\n",
        "        env: GymEnv | str,\n",
        "        learning_rate: float | Schedule = 3e-4,\n",
        "        n_steps: int = 2048,\n",
        "        batch_size: int = 64,\n",
        "        n_epochs: int = 10,\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        clip_range: float | Schedule = 0.2,\n",
        "        clip_range_vf: float | Schedule | None = None,\n",
        "        normalize_advantage: bool = True,\n",
        "        ent_coef: float = 0.0,\n",
        "        vf_coef: float = 0.5,\n",
        "        max_grad_norm: float = 0.5,\n",
        "        use_sde: bool = False,\n",
        "        sde_sample_freq: int = -1,\n",
        "        rollout_buffer_class: type[RolloutBuffer] | None = None,\n",
        "        rollout_buffer_kwargs: dict[str, Any] | None = None,\n",
        "        target_kl: float | None = None,\n",
        "        stats_window_size: int = 100,\n",
        "        tensorboard_log: str | None = None,\n",
        "        policy_kwargs: dict[str, Any] | None = None,\n",
        "        verbose: int = 0,\n",
        "        seed: int | None = None,\n",
        "        device: th.device | str = \"auto\",\n",
        "        _init_setup_model: bool = True,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            policy,\n",
        "            env,\n",
        "            learning_rate=learning_rate,\n",
        "            n_steps=n_steps,\n",
        "            gamma=gamma,\n",
        "            gae_lambda=gae_lambda,\n",
        "            ent_coef=ent_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            use_sde=use_sde,\n",
        "            sde_sample_freq=sde_sample_freq,\n",
        "            rollout_buffer_class=rollout_buffer_class,\n",
        "            rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
        "            stats_window_size=stats_window_size,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            seed=seed,\n",
        "            _init_setup_model=False,\n",
        "            supported_action_spaces=(\n",
        "                spaces.Box,\n",
        "                spaces.Discrete,\n",
        "                spaces.MultiDiscrete,\n",
        "                spaces.MultiBinary,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if normalize_advantage:\n",
        "            assert batch_size > 1, \"`batch_size` must be > 1 when normalize_advantage=True\"\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.clip_range = clip_range\n",
        "        self.clip_range_vf = clip_range_vf\n",
        "        self.normalize_advantage = normalize_advantage\n",
        "        self.target_kl = target_kl\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        super()._setup_model()\n",
        "\n",
        "    def train(self) -> None:\n",
        "        self.policy.set_training_mode(True)\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "\n",
        "        clip_range = (\n",
        "            self.clip_range(self._current_progress_remaining)\n",
        "            if callable(self.clip_range)\n",
        "            else self.clip_range\n",
        "        )\n",
        "\n",
        "        clip_range_vf = None\n",
        "        if self.clip_range_vf is not None:\n",
        "            clip_range_vf = (\n",
        "                self.clip_range_vf(self._current_progress_remaining)\n",
        "                if callable(self.clip_range_vf)\n",
        "                else self.clip_range_vf\n",
        "            )\n",
        "\n",
        "        entropy_losses, pg_losses, value_losses, clip_fractions = [], [], [], []\n",
        "        approx_kl_divs = []\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
        "                actions = rollout_data.actions\n",
        "                if isinstance(self.action_space, spaces.Discrete):\n",
        "                    actions = actions.long().flatten()\n",
        "\n",
        "                values, log_prob, entropy = self.policy.evaluate_actions(\n",
        "                    rollout_data.observations, actions\n",
        "                )\n",
        "                values = values.flatten()\n",
        "\n",
        "                advantages = rollout_data.advantages\n",
        "                if self.normalize_advantage and advantages.numel() > 1:\n",
        "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
        "\n",
        "                policy_loss = -th.mean(\n",
        "                    th.min(\n",
        "                        advantages * ratio,\n",
        "                        advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
        "\n",
        "                if clip_range_vf is None:\n",
        "                    values_pred = values\n",
        "                else:\n",
        "                    values_pred = rollout_data.old_values + th.clamp(\n",
        "                        values - rollout_data.old_values,\n",
        "                        -clip_range_vf,\n",
        "                        clip_range_vf,\n",
        "                    )\n",
        "\n",
        "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
        "\n",
        "                entropy_loss = -th.mean(entropy) if entropy is not None else th.mean(log_prob)\n",
        "\n",
        "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
        "\n",
        "                with th.no_grad():\n",
        "                    approx_kl = th.mean(\n",
        "                        (th.exp(log_prob - rollout_data.old_log_prob) - 1)\n",
        "                        - (log_prob - rollout_data.old_log_prob)\n",
        "                    ).cpu().item()\n",
        "                    approx_kl_divs.append(approx_kl)\n",
        "\n",
        "                if self.target_kl is not None and approx_kl > 1.5 * self.target_kl:\n",
        "                    break\n",
        "\n",
        "                self.policy.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "                self.policy.optimizer.step()\n",
        "\n",
        "                pg_losses.append(policy_loss.item())\n",
        "                value_losses.append(value_loss.item())\n",
        "                entropy_losses.append(entropy_loss.item())\n",
        "                clip_fractions.append(clip_fraction)\n",
        "\n",
        "        explained_var = explained_variance(\n",
        "            self.rollout_buffer.values.flatten(),\n",
        "            self.rollout_buffer.returns.flatten(),\n",
        "        )\n",
        "\n",
        "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
        "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
        "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
        "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
        "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
        "        self.logger.record(\"train/explained_variance\", explained_var)\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "\n",
        "    def learn(\n",
        "        self: SelfPPO,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 1,\n",
        "        tb_log_name: str = \"PPO\",\n",
        "        reset_num_timesteps: bool = True,\n",
        "        progress_bar: bool = False,\n",
        "    ) -> SelfPPO:\n",
        "        return super().learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=callback,\n",
        "            log_interval=log_interval,\n",
        "            tb_log_name=tb_log_name,\n",
        "            reset_num_timesteps=reset_num_timesteps,\n",
        "            progress_bar=progress_bar,\n",
        "        )"
      ],
      "metadata": {
        "id": "2GCNZSZhGjQr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "3wway1fMGrVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class BestModelCallback(BaseCallback):\n",
        "    def __init__(self, model_ref, save_path, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.model_ref = model_ref\n",
        "        self.save_path = save_path\n",
        "        self.best_reward = -np.inf\n",
        "\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        infos = self.locals.get(\"infos\", [])\n",
        "\n",
        "        for info in infos:\n",
        "            if \"episode\" in info:\n",
        "                r = info[\"episode\"][\"r\"]\n",
        "\n",
        "                if r > self.best_reward:\n",
        "                    self.best_reward = r\n",
        "                    self.model_ref.save(self.save_path)\n",
        "\n",
        "                    if self.verbose:\n",
        "                        print(f\"üèÜ New BEST model saved | Reward = {r:.2f}\")\n",
        "\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "CMyYgI5dGxmn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "\n",
        "class EpisodeRewardCallback(BaseCallback):\n",
        "    def __init__(self, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.current_reward = 0\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # reward du step courant\n",
        "        self.current_reward += self.locals[\"rewards\"][0]\n",
        "\n",
        "        # fin d‚Äô√©pisode\n",
        "        if self.locals[\"dones\"][0]:\n",
        "            self.episode_rewards.append(self.current_reward)\n",
        "            self.episode_count += 1\n",
        "\n",
        "            # affichage tous les 10 √©pisodes\n",
        "            if self.episode_count % 10 == 0:\n",
        "                mean_10 = np.mean(self.episode_rewards[-10:])\n",
        "                print(\n",
        "                    f\"√âpisode {self.episode_count} | \"\n",
        "                    f\"Reward √©pisode = {self.current_reward:.2f} | \"\n",
        "                    f\"Moyenne (10) = {mean_10:.2f}\"\n",
        "                )\n",
        "\n",
        "            self.current_reward = 0\n",
        "\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "Z8b9kPvKZ-tu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CleanStatsVideoAndSaveCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_ref,\n",
        "        make_video_env_fn,\n",
        "        print_every=10,\n",
        "        save_every=100,\n",
        "        video_length=1000,\n",
        "        verbose=0\n",
        "    ):\n",
        "        super().__init__(verbose)\n",
        "\n",
        "        self.model_ref = model_ref\n",
        "        self.make_video_env_fn = make_video_env_fn\n",
        "\n",
        "        self.print_every = print_every\n",
        "        self.save_every = save_every\n",
        "        self.video_length = video_length\n",
        "\n",
        "        self.episode_rewards = []\n",
        "\n",
        "        self.base_dir = \"outputs\"\n",
        "        self.video_dir = os.path.join(self.base_dir, \"videos\")\n",
        "        self.graph_dir = os.path.join(self.base_dir, \"graphs\")\n",
        "        self.model_dir = os.path.join(self.base_dir, \"models\")\n",
        "\n",
        "        os.makedirs(self.video_dir, exist_ok=True)\n",
        "        os.makedirs(self.graph_dir, exist_ok=True)\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        for info in self.locals[\"infos\"]:\n",
        "            if \"episode\" in info:\n",
        "                reward = info[\"episode\"][\"r\"]\n",
        "                self.episode_rewards.append(reward)\n",
        "                ep = len(self.episode_rewards)\n",
        "\n",
        "                if ep % self.print_every == 0:\n",
        "                    last = self.episode_rewards[-self.print_every:]\n",
        "                    print(\n",
        "                        f\"[PPO] Episode {ep:4d} | \"\n",
        "                        f\"Mean = {np.mean(last):7.1f} | \"\n",
        "                        f\"Min = {np.min(last):7.1f} | \"\n",
        "                        f\"Max = {np.max(last):7.1f}\"\n",
        "                    )\n",
        "\n",
        "                if ep % self.save_every == 0:\n",
        "                    self.save_graph(ep)\n",
        "                    self.save_model(ep)\n",
        "                    self.record_video(ep)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def save_graph(self, episode):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.episode_rewards, linewidth=2)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.title(\"PPO Training ‚Äî Reward per Episode\")\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "        path = os.path.join(self.graph_dir, f\"reward_curve_ep_{episode}.png\")\n",
        "        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "    def save_model(self, episode):\n",
        "        path = os.path.join(self.model_dir, f\"ppo_model_ep_{episode}\")\n",
        "        self.model_ref.save(path)\n",
        "\n",
        "    def record_video(self, episode):\n",
        "        from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "\n",
        "        env = self.make_video_env_fn()\n",
        "        env = VecVideoRecorder(\n",
        "            env,\n",
        "            video_folder=self.video_dir,\n",
        "            record_video_trigger=lambda x: x == 0,\n",
        "            video_length=self.video_length,\n",
        "            name_prefix=f\"ppo_ep_{episode}\"\n",
        "        )\n",
        "\n",
        "        obs = env.reset()\n",
        "        for _ in range(self.video_length):\n",
        "            action, _ = self.model_ref.predict(obs, deterministic=True)\n",
        "            obs, _, done, _ = env.step(action)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        env.close()\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CleanStatsVideoAndSaveCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_ref,\n",
        "        make_video_env_fn,\n",
        "        print_every=10,\n",
        "        save_every=100,\n",
        "        video_length=1000,\n",
        "        verbose=0\n",
        "    ):\n",
        "        super().__init__(verbose)\n",
        "\n",
        "        self.model_ref = model_ref\n",
        "        self.make_video_env_fn = make_video_env_fn\n",
        "\n",
        "        self.print_every = print_every\n",
        "        self.save_every = save_every\n",
        "        self.video_length = video_length\n",
        "\n",
        "        self.episode_rewards = []\n",
        "\n",
        "        self.base_dir = \"outputs\"\n",
        "        self.video_dir = os.path.join(self.base_dir, \"videos\")\n",
        "        self.graph_dir = os.path.join(self.base_dir, \"graphs\")\n",
        "        self.model_dir = os.path.join(self.base_dir, \"models\")\n",
        "\n",
        "        os.makedirs(self.video_dir, exist_ok=True)\n",
        "        os.makedirs(self.graph_dir, exist_ok=True)\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        for info in self.locals[\"infos\"]:\n",
        "            if \"episode\" in info:\n",
        "                reward = info[\"episode\"][\"r\"]\n",
        "                self.episode_rewards.append(reward)\n",
        "                ep = len(self.episode_rewards)\n",
        "\n",
        "                if ep % self.print_every == 0:\n",
        "                    last = self.episode_rewards[-self.print_every:]\n",
        "                    print(\n",
        "                        f\"[PPO] Episode {ep:4d} | \"\n",
        "                        f\"Mean = {np.mean(last):7.1f} | \"\n",
        "                        f\"Min = {np.min(last):7.1f} | \"\n",
        "                        f\"Max = {np.max(last):7.1f}\"\n",
        "                    )\n",
        "\n",
        "                if ep % self.save_every == 0:\n",
        "                    self.save_graph(ep)\n",
        "                    self.save_model(ep)\n",
        "                    self.record_video(ep)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def save_graph(self, episode):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.episode_rewards, linewidth=2)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.title(\"PPO Training ‚Äî Reward per Episode\")\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "        path = os.path.join(self.graph_dir, f\"reward_curve_ep_{episode}.png\")\n",
        "        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "    def save_model(self, episode):\n",
        "        path = os.path.join(self.model_dir, f\"ppo_model_ep_{episode}\")\n",
        "        self.model_ref.save(path)\n",
        "\n",
        "    def record_video(self, episode):\n",
        "        from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "\n",
        "        env = self.make_video_env_fn()\n",
        "        env = VecVideoRecorder(\n",
        "            env,\n",
        "            video_folder=self.video_dir,\n",
        "            record_video_trigger=lambda x: x == 0,\n",
        "            video_length=self.video_length,\n",
        "            name_prefix=f\"ppo_ep_{episode}\"\n",
        "        )\n",
        "\n",
        "        obs = env.reset()\n",
        "        for _ in range(self.video_length):\n",
        "            action, _ = self.model_ref.predict(obs, deterministic=True)\n",
        "            obs, _, done, _ = env.step(action)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        env.close()\n"
      ],
      "metadata": {
        "id": "-qTq_iFTG2Z6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Envs"
      ],
      "metadata": {
        "id": "XXfLA-z7HGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
        "from stable_baselines3.common.atari_wrappers import WarpFrame\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Global environment configuration\n",
        "# ============================================================\n",
        "ENV_ID = \"CarRacing-v3\"\n",
        "GRAY_SCALE = True\n",
        "N_STACK = 4\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Wrapper selection\n",
        "# ============================================================\n",
        "def _get_wrapper():\n",
        "    \"\"\"\n",
        "    Return the observation wrapper used for preprocessing.\n",
        "    \"\"\"\n",
        "    return WarpFrame if GRAY_SCALE else None\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Training environment\n",
        "# ============================================================\n",
        "def make_train_env(log_dir=None, n_envs=1):\n",
        "    \"\"\"\n",
        "    Create the training environment for PPO.\n",
        "    \"\"\"\n",
        "    wrapper_class = _get_wrapper()\n",
        "\n",
        "    env = make_vec_env(\n",
        "        ENV_ID,\n",
        "        n_envs=n_envs,\n",
        "        wrapper_class=wrapper_class,\n",
        "        monitor_dir=log_dir\n",
        "    )\n",
        "\n",
        "    env = VecFrameStack(env, n_stack=N_STACK)\n",
        "    env = VecTransposeImage(env)\n",
        "\n",
        "    return env\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Evaluation environment\n",
        "# ============================================================\n",
        "def make_eval_env(n_envs=1):\n",
        "    \"\"\"\n",
        "    Environment used for evaluation (no rendering).\n",
        "    \"\"\"\n",
        "    wrapper_class = _get_wrapper()\n",
        "\n",
        "    env = make_vec_env(\n",
        "        ENV_ID,\n",
        "        n_envs=n_envs,\n",
        "        wrapper_class=wrapper_class\n",
        "    )\n",
        "\n",
        "    env = VecFrameStack(env, n_stack=N_STACK)\n",
        "    env = VecTransposeImage(env)\n",
        "\n",
        "    return env\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Video environment\n",
        "# ============================================================\n",
        "def make_video_env():\n",
        "    \"\"\"\n",
        "    Environment used for video recording.\n",
        "    \"\"\"\n",
        "    wrapper_class = _get_wrapper()\n",
        "\n",
        "    env = make_vec_env(\n",
        "        ENV_ID,\n",
        "        n_envs=1,\n",
        "        wrapper_class=wrapper_class\n",
        "    )\n",
        "\n",
        "    env = VecFrameStack(env, n_stack=N_STACK)\n",
        "    env = VecTransposeImage(env)\n",
        "\n",
        "    return env\n"
      ],
      "metadata": {
        "id": "yU4FZvLpHHof"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-qDhZ9DGFZ7"
      },
      "source": [
        "# Experiment Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4CNv4MOyogr0"
      },
      "outputs": [],
      "source": [
        "\n",
        "rl_type = \"PPO\"\n",
        "env_str = \"CarRacing-v3\"\n",
        "\n",
        "log_dir = f\"./logs/{env_str}/{rl_type}\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "total_timesteps = 1_000_000\n",
        "eval_freq = 50_000\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Environment Creation\n",
        "# =========================\n",
        "env = make_train_env(log_dir=log_dir, n_envs=1)\n",
        "env_val = make_eval_env(n_envs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzeRtbSuYNfb",
        "outputId": "f8c6a4c8-e569-4214-eb1c-2a443f9df54f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d-CqP_kseecr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "eval_callback = EvalCallback(\n",
        "    env_val,\n",
        "    best_model_save_path=log_dir,\n",
        "    log_path=log_dir,\n",
        "    eval_freq=eval_freq,\n",
        "    render=False,\n",
        "    deterministic=True,\n",
        "    n_eval_episodes=20\n",
        ")\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=eval_freq,\n",
        "    save_path=os.path.join(log_dir, \"checkpoint\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dSgnPniGFZ7"
      },
      "source": [
        "# Entra√Ænement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X5pyB8FGFZ7"
      },
      "source": [
        "## Entra√Ænement sur 900 √©pisodes. Suivi Vid√©o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9MNxelzKI4S"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    env,\n",
        "    ent_coef=0.0075,\n",
        "    verbose=0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ipORrIFuWjJm",
        "outputId": "126aef7b-5806-44b0-f73e-fba9bf8abbcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PPO] Episode   10 | Mean(last 10) =   -49.3 | Min =   -62.5 | Max =   -39.8\n",
            "[PPO] Episode   20 | Mean(last 10) =   -30.8 | Min =   -57.4 | Max =    -3.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:157: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
            "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PPO] Episode   30 | Mean(last 10) =   -45.2 | Min =   -75.6 | Max =   -17.6\n",
            "[PPO] Episode   40 | Mean(last 10) =     3.4 | Min =   -72.3 | Max =   168.0\n",
            "[PPO] Episode   50 | Mean(last 10) =    59.7 | Min =   -31.2 | Max =   212.7\n",
            "[PPO] Episode   60 | Mean(last 10) =   227.6 | Min =  -125.6 | Max =   480.9\n",
            "[PPO] Episode   70 | Mean(last 10) =   377.7 | Min =    -2.7 | Max =   560.0\n",
            "[PPO] Episode   80 | Mean(last 10) =   314.5 | Min =    50.3 | Max =   542.4\n",
            "[PPO] Episode   90 | Mean(last 10) =   365.1 | Min =   139.0 | Max =   592.0\n",
            "[PPO] Episode  100 | Mean(last 10) =   401.6 | Min =   -24.5 | Max =   788.1\n",
            "üìà Graph saved: reward_curve_ep_100.png\n",
            "üíæ Model saved: ppo_model_ep_100.zip\n",
            "üé¨ Recording video at episode 100\n",
            "Saving video to /content/outputs/videos/ppo_ep_100-step-0-to-step-1000.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video /content/outputs/videos/ppo_ep_100-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_100-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_100-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_100.mp4\n",
            "[PPO] Episode  110 | Mean(last 10) =   498.4 | Min =   225.4 | Max =   850.0\n",
            "[PPO] Episode  120 | Mean(last 10) =   510.7 | Min =   247.4 | Max =   650.9\n",
            "[PPO] Episode  130 | Mean(last 10) =   527.0 | Min =   239.2 | Max =   855.2\n",
            "[PPO] Episode  140 | Mean(last 10) =   482.4 | Min =   401.7 | Max =   583.6\n",
            "[PPO] Episode  150 | Mean(last 10) =   612.2 | Min =   408.9 | Max =   857.6\n",
            "[PPO] Episode  160 | Mean(last 10) =   594.3 | Min =   282.9 | Max =   871.2\n",
            "[PPO] Episode  170 | Mean(last 10) =   494.8 | Min =    92.3 | Max =   901.4\n",
            "[PPO] Episode  180 | Mean(last 10) =   406.2 | Min =   -53.2 | Max =   877.0\n",
            "[PPO] Episode  190 | Mean(last 10) =   566.0 | Min =    -4.6 | Max =   923.7\n",
            "[PPO] Episode  200 | Mean(last 10) =   507.8 | Min =   -18.3 | Max =   783.6\n",
            "üìà Graph saved: reward_curve_ep_200.png\n",
            "üíæ Model saved: ppo_model_ep_200.zip\n",
            "üé¨ Recording video at episode 200\n",
            "Saving video to /content/outputs/videos/ppo_ep_200-step-0-to-step-1000.mp4\n",
            "Moviepy - Building video /content/outputs/videos/ppo_ep_200-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_200-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_200-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_200.mp4\n",
            "[PPO] Episode  210 | Mean(last 10) =   481.3 | Min =   136.5 | Max =   904.6\n",
            "[PPO] Episode  220 | Mean(last 10) =   523.7 | Min =   192.1 | Max =   761.0\n",
            "[PPO] Episode  230 | Mean(last 10) =   394.1 | Min =   135.9 | Max =   701.5\n",
            "[PPO] Episode  240 | Mean(last 10) =   493.6 | Min =   151.7 | Max =   697.8\n",
            "[PPO] Episode  250 | Mean(last 10) =   438.0 | Min =    52.5 | Max =   837.0\n",
            "[PPO] Episode  260 | Mean(last 10) =   639.3 | Min =    71.6 | Max =   861.5\n",
            "[PPO] Episode  270 | Mean(last 10) =   530.8 | Min =   -41.7 | Max =   801.1\n",
            "[PPO] Episode  280 | Mean(last 10) =   613.8 | Min =   345.7 | Max =   810.4\n",
            "[PPO] Episode  290 | Mean(last 10) =   498.6 | Min =   247.2 | Max =   685.5\n",
            "[PPO] Episode  300 | Mean(last 10) =   456.5 | Min =   143.6 | Max =   780.2\n",
            "üìà Graph saved: reward_curve_ep_300.png\n",
            "üíæ Model saved: ppo_model_ep_300.zip\n",
            "üé¨ Recording video at episode 300\n",
            "Saving video to /content/outputs/videos/ppo_ep_300-step-0-to-step-1000.mp4\n",
            "Moviepy - Building video /content/outputs/videos/ppo_ep_300-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_300-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_300-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_300.mp4\n",
            "[PPO] Episode  310 | Mean(last 10) =   281.7 | Min =   128.0 | Max =   571.8\n",
            "[PPO] Episode  320 | Mean(last 10) =   348.6 | Min =    87.9 | Max =   564.2\n",
            "[PPO] Episode  330 | Mean(last 10) =   566.9 | Min =   174.4 | Max =   752.8\n",
            "[PPO] Episode  340 | Mean(last 10) =   587.3 | Min =   288.2 | Max =   808.7\n",
            "[PPO] Episode  350 | Mean(last 10) =   649.3 | Min =   249.7 | Max =   904.5\n",
            "[PPO] Episode  360 | Mean(last 10) =   600.2 | Min =    71.4 | Max =   912.8\n",
            "[PPO] Episode  370 | Mean(last 10) =   750.8 | Min =   407.0 | Max =   905.2\n",
            "[PPO] Episode  380 | Mean(last 10) =   677.2 | Min =    93.5 | Max =   915.9\n",
            "[PPO] Episode  390 | Mean(last 10) =   804.9 | Min =   664.5 | Max =   903.7\n",
            "[PPO] Episode  400 | Mean(last 10) =   686.5 | Min =   369.7 | Max =   849.8\n",
            "üìà Graph saved: reward_curve_ep_400.png\n",
            "üíæ Model saved: ppo_model_ep_400.zip\n",
            "üé¨ Recording video at episode 400\n",
            "Saving video to /content/outputs/videos/ppo_ep_400-step-0-to-step-1000.mp4\n",
            "Moviepy - Building video /content/outputs/videos/ppo_ep_400-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_400-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_400-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_400.mp4\n",
            "[PPO] Episode  410 | Mean(last 10) =   784.7 | Min =   575.7 | Max =   889.7\n",
            "[PPO] Episode  420 | Mean(last 10) =   606.8 | Min =    57.7 | Max =   906.7\n",
            "[PPO] Episode  430 | Mean(last 10) =   661.1 | Min =   523.6 | Max =   838.1\n",
            "[PPO] Episode  440 | Mean(last 10) =   543.2 | Min =   395.4 | Max =   918.0\n",
            "[PPO] Episode  450 | Mean(last 10) =   682.9 | Min =   319.6 | Max =   874.8\n",
            "[PPO] Episode  460 | Mean(last 10) =   740.9 | Min =   450.0 | Max =   889.9\n",
            "[PPO] Episode  470 | Mean(last 10) =   740.3 | Min =   313.2 | Max =   892.6\n",
            "[PPO] Episode  480 | Mean(last 10) =   785.9 | Min =   649.0 | Max =   875.0\n",
            "[PPO] Episode  490 | Mean(last 10) =   860.8 | Min =   720.8 | Max =   915.3\n",
            "[PPO] Episode  500 | Mean(last 10) =   801.0 | Min =   416.9 | Max =   888.8\n",
            "üìà Graph saved: reward_curve_ep_500.png\n",
            "üíæ Model saved: ppo_model_ep_500.zip\n",
            "üé¨ Recording video at episode 500\n",
            "Saving video to /content/outputs/videos/ppo_ep_500-step-0-to-step-1000.mp4\n",
            "Moviepy - Building video /content/outputs/videos/ppo_ep_500-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_500-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_500-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_500.mp4\n",
            "[PPO] Episode  510 | Mean(last 10) =   734.5 | Min =    77.6 | Max =   893.5\n",
            "[PPO] Episode  520 | Mean(last 10) =   821.2 | Min =   666.8 | Max =   864.5\n",
            "[PPO] Episode  530 | Mean(last 10) =   860.8 | Min =   766.7 | Max =   908.0\n",
            "[PPO] Episode  540 | Mean(last 10) =   680.3 | Min =    31.8 | Max =   923.6\n",
            "[PPO] Episode  550 | Mean(last 10) =   770.4 | Min =   366.9 | Max =   911.0\n",
            "[PPO] Episode  560 | Mean(last 10) =   778.4 | Min =   528.1 | Max =   910.3\n",
            "[PPO] Episode  570 | Mean(last 10) =   808.5 | Min =   499.4 | Max =   920.1\n",
            "[PPO] Episode  580 | Mean(last 10) =   783.5 | Min =   375.6 | Max =   923.2\n",
            "[PPO] Episode  590 | Mean(last 10) =   808.0 | Min =   561.9 | Max =   877.7\n",
            "[PPO] Episode  600 | Mean(last 10) =   717.3 | Min =   370.8 | Max =   887.5\n",
            "üìà Graph saved: reward_curve_ep_600.png\n",
            "üíæ Model saved: ppo_model_ep_600.zip\n",
            "üé¨ Recording video at episode 600\n",
            "Saving video to /content/outputs/videos/ppo_ep_600-step-0-to-step-1000.mp4\n",
            "Moviepy - Building video /content/outputs/videos/ppo_ep_600-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_600-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_600-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_600.mp4\n",
            "[PPO] Episode  610 | Mean(last 10) =   737.3 | Min =   462.5 | Max =   871.7\n",
            "[PPO] Episode  620 | Mean(last 10) =   626.6 | Min =   373.7 | Max =   864.7\n",
            "[PPO] Episode  630 | Mean(last 10) =   610.5 | Min =   358.7 | Max =   850.7\n",
            "[PPO] Episode  640 | Mean(last 10) =   616.9 | Min =    94.5 | Max =   917.3\n",
            "[PPO] Episode  650 | Mean(last 10) =   281.4 | Min =   -66.4 | Max =   855.6\n",
            "[PPO] Episode  660 | Mean(last 10) =   787.2 | Min =   442.1 | Max =   893.2\n",
            "[PPO] Episode  670 | Mean(last 10) =   772.7 | Min =   574.9 | Max =   919.2\n",
            "[PPO] Episode  680 | Mean(last 10) =   858.6 | Min =   781.8 | Max =   904.7\n",
            "[PPO] Episode  690 | Mean(last 10) =   867.6 | Min =   763.1 | Max =   914.1\n",
            "[PPO] Episode  700 | Mean(last 10) =   569.4 | Min =    56.4 | Max =   892.8\n",
            "üìà Graph saved: reward_curve_ep_700.png\n",
            "üíæ Model saved: ppo_model_ep_700.zip\n",
            "üé¨ Recording video at episode 700\n",
            "Moviepy - Building video /content/outputs/videos/ppo_ep_700-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_700-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_700-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_700.mp4\n",
            "[PPO] Episode  710 | Mean(last 10) =   851.8 | Min =   483.9 | Max =   911.6\n",
            "[PPO] Episode  720 | Mean(last 10) =   581.7 | Min =    39.3 | Max =   911.0\n",
            "[PPO] Episode  730 | Mean(last 10) =   699.7 | Min =   245.8 | Max =   915.3\n",
            "[PPO] Episode  740 | Mean(last 10) =   703.3 | Min =   -30.5 | Max =   914.6\n",
            "[PPO] Episode  750 | Mean(last 10) =   602.7 | Min =    79.7 | Max =   914.0\n",
            "[PPO] Episode  760 | Mean(last 10) =   506.7 | Min =   -60.3 | Max =   798.6\n",
            "[PPO] Episode  770 | Mean(last 10) =   621.9 | Min =    34.3 | Max =   848.6\n",
            "[PPO] Episode  780 | Mean(last 10) =   752.9 | Min =   481.8 | Max =   873.2\n",
            "[PPO] Episode  790 | Mean(last 10) =   683.3 | Min =    54.4 | Max =   884.1\n",
            "[PPO] Episode  800 | Mean(last 10) =   670.5 | Min =    60.7 | Max =   914.8\n",
            "üìà Graph saved: reward_curve_ep_800.png\n",
            "üíæ Model saved: ppo_model_ep_800.zip\n",
            "üé¨ Recording video at episode 800\n",
            "Saving video to /content/outputs/videos/ppo_ep_800-step-0-to-step-1000.mp4\n",
            "Moviepy - Building video /content/outputs/videos/ppo_ep_800-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_800-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_800-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_800.mp4\n",
            "[PPO] Episode  810 | Mean(last 10) =   800.4 | Min =   606.6 | Max =   888.8\n",
            "[PPO] Episode  820 | Mean(last 10) =   782.5 | Min =   261.1 | Max =   910.9\n",
            "[PPO] Episode  830 | Mean(last 10) =   848.2 | Min =   570.1 | Max =   923.3\n",
            "[PPO] Episode  840 | Mean(last 10) =   767.6 | Min =    70.6 | Max =   902.0\n",
            "[PPO] Episode  850 | Mean(last 10) =   704.1 | Min =    58.1 | Max =   917.0\n",
            "[PPO] Episode  860 | Mean(last 10) =   778.6 | Min =   328.6 | Max =   892.8\n",
            "[PPO] Episode  870 | Mean(last 10) =   702.6 | Min =   -59.7 | Max =   889.4\n",
            "[PPO] Episode  880 | Mean(last 10) =   698.1 | Min =   335.5 | Max =   892.7\n",
            "[PPO] Episode  890 | Mean(last 10) =   772.4 | Min =   266.2 | Max =   921.8\n",
            "[PPO] Episode  900 | Mean(last 10) =   668.1 | Min =   202.2 | Max =   925.7\n",
            "üìà Graph saved: reward_curve_ep_900.png\n",
            "üíæ Model saved: ppo_model_ep_900.zip\n",
            "üé¨ Recording video at episode 900\n",
            "Moviepy - Building video /content/outputs/videos/ppo_ep_900-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/outputs/videos/ppo_ep_900-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/outputs/videos/ppo_ep_900-step-0-to-step-1000.mp4\n",
            "‚úÖ Video saved: ppo_ep_900.mp4\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4288603171.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m model.learn(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1_000_000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;31m# Optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# Clip grad norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "callback = CleanStatsVideoAndSaveCallback(\n",
        "    model_ref=model,\n",
        "    make_video_env_fn=make_video_env,\n",
        "    print_every=10,\n",
        "    save_every=100,\n",
        "    video_length=1000\n",
        ")\n",
        "\n",
        "\n",
        "model.learn(\n",
        "    total_timesteps=1_000_000,\n",
        "    callback=callback,\n",
        "    reset_num_timesteps=False,\n",
        "    progress_bar=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tlYiAdHGFZ8"
      },
      "source": [
        "## Entrainement Best Model Callback\n",
        "\n",
        "La m√©thode load permet de recharger l‚Äôagent PPO exactement dans l‚Äô√©tat o√π il a √©t√© sauvegard√©, afin de poursuivre l‚Äôentra√Ænement ou d‚Äô√©valuer sa politique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbdDH4moGLJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f75cb8-4010-402c-85a8-7571defb6ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = PPO.load(\n",
        "     \"/content/ppo_model_ep_900.zip\",\n",
        "     env=env\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDtee9wUGOGu"
      },
      "outputs": [],
      "source": [
        "\n",
        " best_callback = BestModelCallback(\n",
        "     model_ref=model,\n",
        "     save_path=\"/content/outputs/models/BEST_MODEL_FINAL.zip\"\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTRBB0wNCgmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tentative de r√©aintrenement apr√®s 900 √©pisodes.\n",
        "\n",
        "Malheuresement la session a crash apr√®s 7h d'entrainement et j'ai perdu tous mon GPU."
      ],
      "metadata": {
        "id": "XMTtmMUJCiwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CleanStatsAndSaveCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_ref,\n",
        "        print_every=20,\n",
        "        save_every=100,\n",
        "        verbose=0\n",
        "    ):\n",
        "        super().__init__(verbose)\n",
        "\n",
        "        self.model_ref = model_ref\n",
        "        self.print_every = print_every\n",
        "        self.save_every = save_every\n",
        "\n",
        "        self.episode_rewards = []\n",
        "\n",
        "        self.base_dir = \"outputs\"\n",
        "        self.graph_dir = os.path.join(self.base_dir, \"graphs\")\n",
        "        self.model_dir = os.path.join(self.base_dir, \"models\")\n",
        "\n",
        "        os.makedirs(self.graph_dir, exist_ok=True)\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        for info in self.locals[\"infos\"]:\n",
        "            if \"episode\" in info:\n",
        "                reward = info[\"episode\"][\"r\"]\n",
        "                self.episode_rewards.append(reward)\n",
        "                ep = len(self.episode_rewards)\n",
        "\n",
        "                # ===== affichage stats =====\n",
        "                if ep % self.print_every == 0:\n",
        "                    last = self.episode_rewards[-self.print_every:]\n",
        "                    print(\n",
        "                        f\"[PPO] Episode {ep:4d} | \"\n",
        "                        f\"Mean = {np.mean(last):7.1f} | \"\n",
        "                        f\"Min = {np.min(last):7.1f} | \"\n",
        "                        f\"Max = {np.max(last):7.1f}\"\n",
        "                    )\n",
        "\n",
        "                # ===== sauvegardes =====\n",
        "                if ep % self.save_every == 0:\n",
        "                    self.save_graph(ep)\n",
        "                    self.save_model(ep)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def save_graph(self, episode):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.episode_rewards, linewidth=2)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.title(\"PPO Training ‚Äî Reward per Episode\")\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "        path = os.path.join(self.graph_dir, f\"reward_curve_ep_{episode}.png\")\n",
        "        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "    def save_model(self, episode):\n",
        "        path = os.path.join(self.model_dir, f\"ppo_model_ep_{episode}\")\n",
        "        self.model_ref.save(path)\n"
      ],
      "metadata": {
        "id": "QmgcFS7tzUAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp6QsZmFGROB",
        "outputId": "81d0a035-813c-42c1-8e25-13d41e7b5fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÜ New BEST model saved | Reward = 11.86\n",
            "üèÜ New BEST model saved | Reward = 874.82\n",
            "üèÜ New BEST model saved | Reward = 875.52\n",
            "üèÜ New BEST model saved | Reward = 904.20\n",
            "üèÜ New BEST model saved | Reward = 911.60\n",
            "[PPO] Episode   20 | Mean =   562.8 | Min =    11.9 | Max =   911.6\n",
            "üèÜ New BEST model saved | Reward = 918.60\n",
            "üèÜ New BEST model saved | Reward = 922.40\n",
            "üèÜ New BEST model saved | Reward = 923.00\n",
            "[PPO] Episode   40 | Mean =   635.8 | Min =   -11.6 | Max =   923.0\n",
            "[PPO] Episode   60 | Mean =   491.5 | Min =   -32.9 | Max =   888.1\n",
            "[PPO] Episode   80 | Mean =   597.0 | Min =    29.3 | Max =   913.3\n",
            "[PPO] Episode  100 | Mean =   648.1 | Min =    51.6 | Max =   921.7\n",
            "[PPO] Episode  120 | Mean =   718.4 | Min =   278.3 | Max =   909.0\n",
            "üèÜ New BEST model saved | Reward = 923.30\n",
            "[PPO] Episode  140 | Mean =   632.8 | Min =    83.2 | Max =   923.3\n",
            "[PPO] Episode  160 | Mean =   456.5 | Min =    34.8 | Max =   892.3\n",
            "[PPO] Episode  180 | Mean =   315.3 | Min =   -27.8 | Max =   772.7\n",
            "[PPO] Episode  200 | Mean =   418.0 | Min =     9.0 | Max =   889.8\n",
            "üèÜ New BEST model saved | Reward = 929.80\n",
            "[PPO] Episode  220 | Mean =   424.5 | Min =     0.3 | Max =   929.8\n",
            "[PPO] Episode  240 | Mean =   410.7 | Min =    61.0 | Max =   883.2\n",
            "[PPO] Episode  260 | Mean =   480.9 | Min =    61.7 | Max =   876.5\n",
            "[PPO] Episode  280 | Mean =   577.0 | Min =    98.0 | Max =   916.2\n",
            "[PPO] Episode  300 | Mean =   628.3 | Min =    -7.2 | Max =   929.2\n",
            "[PPO] Episode  320 | Mean =   600.1 | Min =    86.0 | Max =   906.0\n",
            "[PPO] Episode  340 | Mean =   605.5 | Min =   208.7 | Max =   917.5\n",
            "[PPO] Episode  360 | Mean =   575.1 | Min =   110.9 | Max =   896.0\n",
            "[PPO] Episode  380 | Mean =   560.7 | Min =   105.4 | Max =   907.3\n",
            "[PPO] Episode  400 | Mean =   518.4 | Min =    63.2 | Max =   911.6\n",
            "[PPO] Episode  420 | Mean =   561.2 | Min =   305.6 | Max =   889.0\n",
            "[PPO] Episode  440 | Mean =   683.4 | Min =   241.3 | Max =   896.6\n",
            "[PPO] Episode  460 | Mean =   648.8 | Min =   288.1 | Max =   879.9\n",
            "[PPO] Episode  480 | Mean =   467.8 | Min =    16.7 | Max =   904.6\n",
            "[PPO] Episode  500 | Mean =   622.7 | Min =    53.8 | Max =   911.8\n",
            "[PPO] Episode  520 | Mean =   557.7 | Min =     6.0 | Max =   876.8\n",
            "[PPO] Episode  540 | Mean =   602.7 | Min =    25.8 | Max =   926.4\n",
            "[PPO] Episode  560 | Mean =   551.8 | Min =   219.1 | Max =   896.5\n",
            "[PPO] Episode  580 | Mean =   524.7 | Min =   194.5 | Max =   885.0\n",
            "[PPO] Episode  600 | Mean =   655.9 | Min =    34.2 | Max =   926.8\n",
            "[PPO] Episode  620 | Mean =   788.5 | Min =   289.5 | Max =   893.2\n",
            "[PPO] Episode  640 | Mean =   701.7 | Min =   263.0 | Max =   920.7\n",
            "[PPO] Episode  660 | Mean =   641.7 | Min =     6.8 | Max =   911.0\n",
            "üèÜ New BEST model saved | Reward = 929.90\n",
            "[PPO] Episode  680 | Mean =   722.4 | Min =   165.0 | Max =   929.9\n",
            "[PPO] Episode  700 | Mean =   724.2 | Min =    28.1 | Max =   926.0\n",
            "[PPO] Episode  720 | Mean =   732.6 | Min =   324.4 | Max =   924.6\n",
            "[PPO] Episode  740 | Mean =   734.6 | Min =   117.5 | Max =   921.4\n",
            "üèÜ New BEST model saved | Reward = 931.20\n",
            "[PPO] Episode  760 | Mean =   766.2 | Min =    65.4 | Max =   931.2\n",
            "[PPO] Episode  780 | Mean =   728.6 | Min =    77.9 | Max =   927.7\n",
            "[PPO] Episode  800 | Mean =   684.3 | Min =   246.8 | Max =   927.9\n",
            "[PPO] Episode  820 | Mean =   821.6 | Min =   322.2 | Max =   926.0\n",
            "üèÜ New BEST model saved | Reward = 932.70\n",
            "[PPO] Episode  840 | Mean =   812.8 | Min =   259.2 | Max =   932.7\n",
            "[PPO] Episode  860 | Mean =   685.5 | Min =    93.0 | Max =   922.9\n",
            "[PPO] Episode  880 | Mean =   746.1 | Min =   -56.5 | Max =   918.7\n",
            "[PPO] Episode  900 | Mean =   827.3 | Min =   409.7 | Max =   923.6\n",
            "[PPO] Episode  920 | Mean =   722.6 | Min =   126.6 | Max =   931.0\n",
            "[PPO] Episode  940 | Mean =   816.3 | Min =   473.7 | Max =   922.3\n",
            "[PPO] Episode  960 | Mean =   846.3 | Min =   661.9 | Max =   930.4\n",
            "[PPO] Episode  980 | Mean =   830.5 | Min =   620.0 | Max =   908.5\n",
            "[PPO] Episode 1000 | Mean =   725.8 | Min =    57.0 | Max =   922.4\n",
            "[PPO] Episode 1020 | Mean =   858.7 | Min =   539.8 | Max =   927.6\n",
            "[PPO] Episode 1040 | Mean =   813.3 | Min =   204.1 | Max =   921.1\n",
            "üèÜ New BEST model saved | Reward = 934.00\n",
            "[PPO] Episode 1060 | Mean =   840.4 | Min =   465.8 | Max =   934.0\n",
            "[PPO] Episode 1080 | Mean =   744.3 | Min =   294.5 | Max =   931.4\n",
            "[PPO] Episode 1100 | Mean =   828.8 | Min =   555.3 | Max =   931.1\n",
            "[PPO] Episode 1120 | Mean =   781.3 | Min =   320.3 | Max =   927.7\n",
            "[PPO] Episode 1140 | Mean =   764.1 | Min =   370.8 | Max =   892.0\n",
            "üèÜ New BEST model saved | Reward = 935.20\n",
            "[PPO] Episode 1160 | Mean =   865.0 | Min =   619.9 | Max =   935.2\n",
            "[PPO] Episode 1180 | Mean =   814.1 | Min =   155.1 | Max =   921.1\n",
            "[PPO] Episode 1200 | Mean =   836.7 | Min =   437.3 | Max =   922.5\n",
            "[PPO] Episode 1220 | Mean =   773.3 | Min =   407.3 | Max =   928.1\n",
            "[PPO] Episode 1240 | Mean =   689.4 | Min =    19.3 | Max =   929.0\n",
            "[PPO] Episode 1260 | Mean =   794.2 | Min =    14.6 | Max =   915.7\n",
            "[PPO] Episode 1280 | Mean =   757.2 | Min =   393.2 | Max =   896.3\n",
            "[PPO] Episode 1300 | Mean =   724.5 | Min =   199.3 | Max =   914.7\n",
            "[PPO] Episode 1320 | Mean =   775.8 | Min =   334.6 | Max =   918.6\n",
            "[PPO] Episode 1340 | Mean =   690.2 | Min =    35.1 | Max =   924.9\n",
            "[PPO] Episode 1360 | Mean =   762.4 | Min =   271.8 | Max =   934.6\n",
            "[PPO] Episode 1380 | Mean =   785.3 | Min =   291.8 | Max =   932.2\n",
            "[PPO] Episode 1400 | Mean =   785.0 | Min =    25.6 | Max =   935.2\n",
            "[PPO] Episode 1420 | Mean =   745.2 | Min =    67.3 | Max =   924.3\n",
            "[PPO] Episode 1440 | Mean =   845.3 | Min =   261.6 | Max =   932.1\n",
            "[PPO] Episode 1460 | Mean =   809.5 | Min =   454.8 | Max =   917.3\n",
            "üèÜ New BEST model saved | Reward = 936.50\n",
            "[PPO] Episode 1480 | Mean =   730.1 | Min =   -44.9 | Max =   936.5\n",
            "[PPO] Episode 1500 | Mean =   820.8 | Min =   345.2 | Max =   931.0\n",
            "[PPO] Episode 1520 | Mean =   743.2 | Min =   213.1 | Max =   925.8\n",
            "[PPO] Episode 1540 | Mean =   810.6 | Min =   347.2 | Max =   929.6\n",
            "üèÜ New BEST model saved | Reward = 937.00\n",
            "[PPO] Episode 1560 | Mean =   796.7 | Min =    85.4 | Max =   937.0\n",
            "[PPO] Episode 1580 | Mean =   615.0 | Min =   122.2 | Max =   929.0\n",
            "[PPO] Episode 1600 | Mean =   789.4 | Min =   334.5 | Max =   926.9\n",
            "[PPO] Episode 1620 | Mean =   733.8 | Min =   207.9 | Max =   925.5\n",
            "[PPO] Episode 1640 | Mean =   665.9 | Min =    72.3 | Max =   936.3\n",
            "[PPO] Episode 1660 | Mean =   581.0 | Min =   219.9 | Max =   924.0\n",
            "[PPO] Episode 1680 | Mean =   582.3 | Min =   136.2 | Max =   928.3\n",
            "[PPO] Episode 1700 | Mean =   682.3 | Min =   198.4 | Max =   930.0\n",
            "[PPO] Episode 1720 | Mean =   635.5 | Min =    65.6 | Max =   917.1\n",
            "[PPO] Episode 1740 | Mean =   531.4 | Min =    67.2 | Max =   912.4\n",
            "[PPO] Episode 1760 | Mean =   657.3 | Min =   126.1 | Max =   914.7\n",
            "[PPO] Episode 1780 | Mean =   646.8 | Min =   190.5 | Max =   923.9\n",
            "[PPO] Episode 1800 | Mean =   594.0 | Min =    11.9 | Max =   919.7\n",
            "[PPO] Episode 1820 | Mean =   708.0 | Min =   317.6 | Max =   922.7\n",
            "[PPO] Episode 1840 | Mean =   605.4 | Min =     3.4 | Max =   918.8\n",
            "[PPO] Episode 1860 | Mean =   623.1 | Min =    21.4 | Max =   936.6\n",
            "[PPO] Episode 1880 | Mean =   825.7 | Min =   487.6 | Max =   925.2\n",
            "[PPO] Episode 1900 | Mean =   563.5 | Min =   -24.9 | Max =   930.9\n",
            "[PPO] Episode 1920 | Mean =   533.5 | Min =   -76.5 | Max =   904.4\n",
            "[PPO] Episode 1940 | Mean =   727.6 | Min =   304.5 | Max =   923.1\n",
            "[PPO] Episode 1960 | Mean =   694.7 | Min =   121.5 | Max =   927.8\n",
            "[PPO] Episode 1980 | Mean =   802.4 | Min =   520.4 | Max =   923.0\n",
            "[PPO] Episode 2000 | Mean =   672.9 | Min =   347.5 | Max =   878.2\n",
            "[PPO] Episode 2020 | Mean =   823.9 | Min =    10.1 | Max =   925.9\n",
            "[PPO] Episode 2040 | Mean =   778.3 | Min =   457.8 | Max =   923.7\n",
            "[PPO] Episode 2060 | Mean =   748.2 | Min =   523.3 | Max =   931.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.PPO at 0x7d1e5a688500>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "\n",
        "best_callback = BestModelCallback(\n",
        "    model_ref=model,\n",
        "    save_path=\"outputs/models/new\"\n",
        ")\n",
        "\n",
        "stats_callback = CleanStatsAndSaveCallback(\n",
        "    model_ref=model,\n",
        "    print_every=20,\n",
        "    save_every=100\n",
        ")\n",
        "\n",
        "callbacks = CallbackList([\n",
        "    best_callback,\n",
        "    stats_callback\n",
        "])\n",
        "\n",
        "model.learn(\n",
        "    total_timesteps=2_000_000,\n",
        "    callback=callbacks,\n",
        "    reset_num_timesteps=False,\n",
        "    progress_bar=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Malheuresement je n'ai pas pu enregistr√© le mod√®le final..."
      ],
      "metadata": {
        "id": "8gkUsqM9g2A4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma-o8mk9GFZ9"
      },
      "source": [
        "## G√©n√©rer une vid√©o finale du meilleur mod√®le."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "-Vhxg-jnGujx",
        "outputId": "c972742a-b721-401c-e933-9c7c6274e1e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2632429574.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \"\"\"\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    363\u001b[0m             )\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mobs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mobs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/utils.py\u001b[0m in \u001b[0;36mobs_as_tensor\u001b[0;34m(obs, device)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# ================================\n",
        "# FINAL BEST MODEL VIDEO (30s)\n",
        "# ================================\n",
        "\n",
        "# Pour tester mes r√©sultats charger le mod√®le sur Colllab et executer les cellules\n",
        "#BEST_MODEL_PATH = \"/content/outputs/models/BEST_MODEL_FINAL.zip\"\n",
        "BEST_MODEL_PATH = \"BEST_MODEL_FINAL.zip\"\n",
        "VIDEO_DIR = \"/content/outputs/videos\"\n",
        "VIDEO_NAME = \"BEST_MODEL_FINAL\"\n",
        "\n",
        "model = PPO.load(\n",
        "    BEST_MODEL_PATH,\n",
        "    env=make_video_env()\n",
        ")\n",
        "\n",
        "env = make_video_env()\n",
        "env = VecVideoRecorder(\n",
        "    env,\n",
        "    video_folder=VIDEO_DIR,\n",
        "    record_video_trigger=lambda x: True,\n",
        "    video_length=900,\n",
        "    name_prefix=VIDEO_NAME\n",
        ")\n",
        "\n",
        "obs = env.reset()\n",
        "for _ in range(900):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, done, _ = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n",
        "video_path = os.path.join(\n",
        "    VIDEO_DIR,\n",
        "    f\"{VIDEO_NAME}-step-0-to-step-900.mp4\"\n",
        ")\n",
        "print(\" Final video generated:\", video_path)\n",
        "\n",
        "files.download(video_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEmVnWY8K-wt"
      },
      "source": [
        "# FINAL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izJukW9OetRh",
        "outputId": "f79f8b0c-b0d9-4960-f071-8913e85d2b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Final BEST Model Evaluation\n",
            "Mean reward: 831.42\n",
            "Std reward : 165.20\n",
            "Final model saved to: ./logs/CarRacing-v3/PPO/final_model_800plus.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import os\n",
        "\n",
        "# ================================\n",
        "# LOAD BEST MODEL (OBJECTIVE)\n",
        "# ================================\n",
        "\n",
        "# Version locale (par d√©faut)\n",
        "BEST_MODEL_PATH = \"BEST_MODEL_FINAL.zip\"\n",
        "\n",
        "# Version Colab\n",
        "# BEST_MODEL_PATH = \"/content/outputs/models/BEST_MODEL_FINAL.zip\"\n",
        "\n",
        "model = PPO.load(\n",
        "    BEST_MODEL_PATH,\n",
        "    env=env_val\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# FINAL EVALUATION\n",
        "# ================================\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model,\n",
        "    env_val,\n",
        "    n_eval_episodes=20,\n",
        "    deterministic=True\n",
        ")\n",
        "\n",
        "print(\" Final BEST Model Evaluation\")\n",
        "print(f\"Mean reward: {mean_reward:.2f}\")\n",
        "print(f\"Std reward : {std_reward:.2f}\")\n",
        "\n",
        "# ================================\n",
        "# SAVE FINAL MODEL (CLEAN NAME)\n",
        "# ================================\n",
        "\n",
        "FINAL_MODEL_PATH = os.path.join(log_dir, \"final_model_800plus\")\n",
        "model.save(FINAL_MODEL_PATH)\n",
        "\n",
        "print(\"Final model saved to:\", FINAL_MODEL_PATH + \".zip\")\n",
        "\n",
        "# ================================\n",
        "# CLEANUP\n",
        "# ================================\n",
        "\n",
        "env.close()\n",
        "env_val.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "TiGj6P-NGXIp",
        "3wway1fMGrVo",
        "XXfLA-z7HGV8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}